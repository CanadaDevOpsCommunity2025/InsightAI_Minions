# alerting-rules.yaml
groups:
  - name: vllm-alerts
    rules:
      - alert: HighPromptTokenRate
        expr: rate(vllm:prompt_tokens_total{model_name="gpt2"}[30s]) > 20
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "ðŸš¨ High token rate detected for gpt2"
          description: "The prompt token rate for gpt2 exceeded 20 tokens/sec over the last 30 seconds."

      - alert: VeryHighPromptTokenRate
        expr: rate(vllm:prompt_tokens_total{model_name="gpt2"}[30s]) > 100
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "ðŸ”¥ CRITICAL: Very high token rate for gpt2"
          description: "The prompt token rate for gpt2 exceeded 100 tokens/sec over the last 30 seconds."
